{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import os\n",
    "\n",
    "dir = ...\n",
    "os.chdir(dir)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from model import DigitClassification, LoRaParametrisation, LoRa_model, enable_disable_lora\n",
    "from datasets import Datasets\n",
    "from trainer import Trainer\n",
    "from paths import *\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = create_results_dir(dir_root = dir)\n",
    "sub_dirs = [\"checkpoints\", \"results\"]\n",
    "dic_pth = {}\n",
    "for sub_ in sub_dirs:\n",
    "    s_path = os.path.join(save_dir, sub_)\n",
    "    dic_pth[sub_] = s_path\n",
    "    os.makedirs(os.path.join(save_dir, sub_), exist_ok = True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load training as testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dataset = Datasets(dataset_name = \"MNIST\")\n",
    "# train set\n",
    "train_loader = dataset.get_dataloader(batch_size = 16)\n",
    "# test set\n",
    "test_loader = dataset.get_dataloader(batch_size = 16, train_status = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = iter(train_loader)\n",
    "images, labels = next(iterator)\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 28\n",
    "classes = 10\n",
    "model = DigitClassification(input_size = im_size*im_size, output_size = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trainer module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 1e-3\n",
    "save_ckeckpoint = 100 \n",
    "path_checkpoint = dic_pth[\"checkpoints\"]\n",
    "trainer = Trainer(model, train_loader, test_loader, lr = lr, device = device, path_checkpoint=dic_pth[\"checkpoints\"], save_ckeckpoint = save_ckeckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.training_loop(epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Copy weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    original_weights[name] = param.data.clone().detach()\n",
    "    \n",
    "\n",
    "# Make a deep copy of the model\n",
    "L_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters of the model: 2807010\n"
     ]
    }
   ],
   "source": [
    "# count parameters in the model\n",
    "def count_parameters(model):\n",
    "    tparas = 0\n",
    "    for layer in model.layers:\n",
    "        tparas += layer.weight.nelement() + layer.bias.nelement()\n",
    "    return tparas\n",
    "\n",
    "print(f\"Total trainable parameters of the model: {count_parameters(L_model)}\") #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784000 1000\n",
      "2000000 2000\n",
      "20000 10\n"
     ]
    }
   ],
   "source": [
    "for layer in L_model.layers:\n",
    "    print(layer.weight.nelement(), layer.bias.nelement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LoRa model**\n",
    "\n",
    "We first load the specific dataset on which we would like to fine tuning our model. By setting `exclude_tgs =9` in `Dataset`, we are specifying that the fine tuning is on the digit `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "dataset_9 = Datasets(dataset_name = \"MNIST\", exclude_tgs=9)\n",
    "# train set\n",
    "train_loader_9 = dataset_9.get_dataloader(batch_size = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct the LoRa model. This consists of freezing all the parameters of the orignial model while creating for each layer, 2 low rank matrices whose parameters will be learned shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers frozen: 6\n",
      "Layer 1 -- Original Weights: 784000  Bias: 1000 ---- Lora_A weight: 2000 + Lora_B weight: 1568\n",
      "Layer 2 -- Original Weights: 2000000  Bias: 2000 ---- Lora_A weight: 4000 + Lora_B weight: 2000\n",
      "Layer 3 -- Original Weights: 20000  Bias: 10 ---- Lora_A weight: 20 + Lora_B weight: 4000\n",
      "Total trainable parameters of the model: 2807010 (non-LoRa) vs 13588 (LoRa) Ratio: 0.48% of the original model\n"
     ]
    }
   ],
   "source": [
    "L_model = LoRa_model(L_model, rank = 2, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trainer for LoRa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 1e-3\n",
    "save_ckeckpoint = 100 \n",
    "path_checkpoint = dic_pth[\"checkpoints\"]\n",
    "trainer_LoRa = Trainer(L_model, train_loader_9, test_loader, lr = lr, device = device, path_checkpoint=dic_pth[\"checkpoints\"], save_ckeckpoint = save_ckeckpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_LoRa.training_loop(epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.200552679777146, 0.5629)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_LoRa.test_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified 975 times for digit 0\n",
      "Misclassified 10 times for digit 1\n",
      "Misclassified 556 times for digit 2\n",
      "Misclassified 454 times for digit 3\n",
      "Misclassified 627 times for digit 4\n",
      "Misclassified 451 times for digit 5\n",
      "Misclassified 140 times for digit 6\n",
      "Misclassified 598 times for digit 7\n",
      "Misclassified 559 times for digit 8\n",
      "Misclassified 1 times for digit 9\n"
     ]
    }
   ],
   "source": [
    "for ii, w in enumerate(trainer_LoRa.wrong_counts):\n",
    "    print(f\"Misclassified {w} times for digit {ii}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\n",
      "parametrizations.weight.original\n",
      "parametrizations.weight.0.lora_A\n",
      "parametrizations.weight.0.lora_B\n",
      "bias\n",
      "parametrizations.weight.original\n",
      "parametrizations.weight.0.lora_A\n",
      "parametrizations.weight.0.lora_B\n",
      "bias\n",
      "parametrizations.weight.original\n",
      "parametrizations.weight.0.lora_A\n",
      "parametrizations.weight.0.lora_B\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(L_model, enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitClassification(\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (layers): ModuleList(\n",
       "    (0): ParametrizedLinear(\n",
       "      in_features=784, out_features=1000, bias=True\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): LoRaParametrisation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ParametrizedLinear(\n",
       "      in_features=1000, out_features=2000, bias=True\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): LoRaParametrisation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ParametrizedLinear(\n",
       "      in_features=2000, out_features=10, bias=True\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): LoRaParametrisation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitClassification(\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=2000, bias=True)\n",
       "    (2): Linear(in_features=2000, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "horus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
